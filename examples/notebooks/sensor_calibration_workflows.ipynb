{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration workflows\n",
    "\n",
    "This notebook shows how to perform calibration based on sensor test data, export and load various types of models. The main implementation is based on sklearn's models and makes use of the fit/predict/transform convention to generalise the structure applied for sensor's processing. \n",
    "\n",
    "These flows can later on be implemented to process sensors' data automatically by using blueprints, simply naming the metric to add (see processing_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scdata.test import Test\n",
    "from scdata.device import Device\n",
    "from scdata._config import config\n",
    "\n",
    "config.out_level='DEBUG'\n",
    "config.framework='jupyterlab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Test('EXAMPLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many devices as needed. See understanding blueprints below for more info\n",
    "test.add_device(Device(blueprint = 'sc_21_station', descriptor = {'source': 'api', \n",
    "                                                              'id': '10751', \n",
    "                                                              'min_date': '2020-05-05'\n",
    "                                                             }\n",
    "                         )\n",
    "               )\n",
    "\n",
    "# Add as many devices as needed. See understanding blueprints below for more info\n",
    "test.add_device(Device(blueprint = 'sc_21_station', descriptor = {'source': 'api', \n",
    "                                                              'id': '10752', \n",
    "                                                              'min_date': '2020-05-05'\n",
    "                                                             }\n",
    "                         )\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test.create()\n",
    "test.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models\n",
    "\n",
    "This section will go through creating some models that will aim to . As mentioned above, this is entirely based on sklearn's package, so it will make extensive use of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn model tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Extra tools\n",
    "from scdata._config import config\n",
    "from scdata.test.utils import normalise_vbls\n",
    "from scdata.io import model_export, model_load\n",
    "from scdata.utils import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs. \n",
    "# Here we will calibrate temperature of one device based on another's temperature\n",
    "measurand = {'10751': ['EXT_TEMP']} # Ground truth\n",
    "inputs = {'10752': ['TEMP']} # Input\n",
    "variables = {\"measurand\": measurand, \"inputs\": inputs}\n",
    "\n",
    "# Options\n",
    "options = config.model_def_opt\n",
    "print (options)\n",
    "\n",
    "# Prepare options\n",
    "df, refn = test.prepare(measurand, inputs)\n",
    "# Do something else with df if necessary\n",
    "labels, features = normalise_vbls(df, refn)\n",
    "\n",
    "# Train test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
    "                                        test_size = options['test_size'], \n",
    "                                        shuffle = options['shuffle'])\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit - predict\n",
    "model.fit(train_X, train_y)\n",
    "train_yhat = model.predict(train_X)\n",
    "test_yhat = model.predict(test_X)\n",
    "\n",
    "# Diagnose\n",
    "metrics = {'train': get_metrics(train_y, train_yhat),\n",
    "           'test': get_metrics(test_y, test_yhat)}\n",
    "\n",
    "# Export\n",
    "model_export(name = 'LINEAR_TEMPERATURE', model = model, variables = variables, \n",
    "             hyperparameters = None, options = options,\n",
    "             metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the metrics\n",
    "print (metrics['train'])\n",
    "\n",
    "print (metrics['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add the metrics into the test\n",
    "metric = {f'TEMP_CORR': {'process': 'apply_regressor',\n",
    "                           'kwargs': {'model': model,\n",
    "                                      'variables': variables,\n",
    "                                      'options': options}\n",
    "                        }}\n",
    "\n",
    "# Add it and process it\n",
    "test.devices['10752'].add_metric(metric)\n",
    "test.devices['10752'].process(metrics = metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {1: {'devices': '10751',\n",
    "              'channel': 'EXT_TEMP',\n",
    "              'subplot': 1},\n",
    "          2: {'devices': '10752',\n",
    "              'channel': 'TEMP_CORR',\n",
    "              'subplot': 1},\n",
    "          3: {'devices': '10752',\n",
    "              'channel': 'TEMP',\n",
    "              'subplot': 1},           \n",
    "         }\n",
    "\n",
    "options = {\n",
    "            'frequency': '1H'\n",
    "}\n",
    "test.ts_iplot(traces = traces, options = options);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {1: {'devices': '10751',\n",
    "              'channel': 'EXT_TEMP',\n",
    "              'subplot': 1},\n",
    "          2: {'devices': '10752',\n",
    "              'channel': 'TEMP_CORR',\n",
    "              'subplot': 1}         \n",
    "         }\n",
    "\n",
    "options = {\n",
    "            'frequency': '1H'\n",
    "}\n",
    "test.scatter_plot(traces = traces, options = options);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs\n",
    "measurand = {'10751': ['EXT_TEMP']} # Ground truth\n",
    "inputs = {'10752': ['TEMP']} # Input\n",
    "variables = {\"measurand\": measurand, \"inputs\": inputs}\n",
    "\n",
    "# Hyperparameters and options\n",
    "hyperparameters = config.model_hyperparameters['rf']\n",
    "options = config.model_def_opt\n",
    "\n",
    "# This averages the common channels into one, if any\n",
    "options['common_avg'] = True\n",
    "\n",
    "# Prepare options\n",
    "df, refn = test.prepare(measurand, inputs, options)\n",
    "\n",
    "# Do something else with df if necessary\n",
    "labels, features = normalise_vbls(df, refn)\n",
    "\n",
    "# Train test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, \n",
    "                                        test_size = options['test_size'], \n",
    "                                        shuffle = options['shuffle'])\n",
    "\n",
    "# Create model\n",
    "model = RandomForestRegressor(n_estimators = hyperparameters['n_estimators'], \n",
    "                              min_samples_leaf = hyperparameters['min_samples_leaf'], \n",
    "                              oob_score = hyperparameters['oob_score'], \n",
    "                              max_features = hyperparameters['max_features'])\n",
    "\n",
    "# Fit - predict\n",
    "model.fit(train_X, train_y)\n",
    "train_yhat = model.predict(train_X)\n",
    "test_yhat = model.predict(test_X)\n",
    "\n",
    "# Diagnose\n",
    "metrics = {'train': get_metrics(train_y, train_yhat),\n",
    "           'test': get_metrics(test_y, test_yhat)}\n",
    "\n",
    "# Export\n",
    "model_export(name = 'RF_TEMP', model = model, variables = variables, \n",
    "             hyperparameters = hyperparameters, options = options,\n",
    "             metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can add the metrics into the test\n",
    "metric = {f'TEMP_CORR_ML': {'process': 'apply_regressor',\n",
    "                           'kwargs': {'model': model,\n",
    "                                      'variables': variables,\n",
    "                                      'options': options}\n",
    "                        }}\n",
    "\n",
    "# Add it and process it\n",
    "test.devices['10752'].add_metric(metric)\n",
    "test.devices['10752'].process(metrics = metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {1: {'devices': '10751',\n",
    "              'channel': 'EXT_TEMP',\n",
    "              'subplot': 1},\n",
    "          2: {'devices': '10752',\n",
    "              'channel': 'TEMP_CORR',\n",
    "              'subplot': 1},\n",
    "          3: {'devices': '10752',\n",
    "              'channel': 'TEMP_CORR_ML',\n",
    "              'subplot': 1},          \n",
    "          4: {'devices': '10752',\n",
    "              'channel': 'TEMP',\n",
    "              'subplot': 1},           \n",
    "         }\n",
    "\n",
    "options = {\n",
    "            'frequency': '1H'\n",
    "}\n",
    "test.ts_iplot(traces = traces, options = options);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {1: {'devices': ['10751', '10752'],\n",
    "              'channels': ['EXT_TEMP', 'TEMP_CORR'],\n",
    "              'subplot': 1},\n",
    "          2: {'devices': ['10751', '10752'],\n",
    "              'channels': ['EXT_TEMP', 'TEMP_CORR_ML'],\n",
    "              'subplot': 2}    \n",
    "        }\n",
    "\n",
    "\n",
    "options = {'frequency': '1H'}\n",
    "formatting = {'width': 25, 'height': 10, 'ylabel': {1: 'Corrected temperature (degC)'}, \n",
    "              'title': 'Alphadelta / Avda Roma - Traffic',\n",
    "                                         'xlabel': {1: 'Ground trugh (degC)'}, \n",
    "              'fontsize': 12}\n",
    "\n",
    "test.scatter_plot(traces = traces, options = options, formatting = formatting);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
